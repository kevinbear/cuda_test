1.----------------------------------------------------------------------
(1) 區分主機和裝置的不同：
   【主機】就是PC。
   【裝置】就是顯示卡。

(2) 兩者皆有【中央處理器】，主機上為 CPU，裝置上為 GPU，指令集不同：
    主機上的程式碼使用傳統 C/C++ 語法撰寫成，實作與呼叫和一般函式無異，
    裝置上的程式碼稱為【核心】(kernel)，需使用 CUDA 的延伸語法 (函式前加
    __global__ 等標籤) 來撰寫，並於呼叫時指定執行緒群組大小 (詳見第三招)

(3) 兩者皆有【各自的記憶體】(DRAM)，擁有獨立的定址空間：
    主機上的透過 malloc()、free()、new、delete 等函式配置與釋放，
    裝置上的透過 cudaMalloc()、cudaFree() 等 API 配置與釋放，
    主機和裝置之間的資料搬移，使用 cudaMemcpy() 這個 API (詳見第二招)

(4) 因為主機和裝置的不同，C/C++ 的標準函式庫不能在 kernel 中直接使用，
    例如要秀出計算結果，必需使用 cudaMemcpy() 先將資料搬移至主機，
    再呼叫 printf 或 cout 等標準輸出函式。

(5) 使用時先在主機記憶體設好資料的初始值，然後傳入裝置記憶體，接著執行核心，
    如果可以的話就儘量讓資料保留在裝置中，進行一連串的 kernel 操作，
    避免透過 PCI-E 搬移造成效能下降，最後再將結果傳回主機中顯示。
------------------------------------------------------------------------

2.----------------------------------------------------------------------
最基本的 API 有 5 個
    (1)配置裝置記憶體   cudaMalloc()            [cuda.h]
    (2)釋放裝置記憶體   cudaFree()              [cuda.h]
    (3)記憶體複制       cudaMemcpy()            [cuda.h]
    (4)錯誤字串解譯     cudaGetErrorString()    [cuda.h]
    (5)同步化           cudaThreadSynchronize() [cuda.h]

用法如下

--------------------------------------------------------
(1)配置顯示記憶體 cudaMalloc()      [cuda.h]
--------------------------------------------------------

       cudaError_t cudaMalloc(void** ptr, size_t count);

           ptr   指向目的指位器之位址
           count 欲配置的大小(單位 bytes)

       傳回值 cudaError_t 是個 enum, 執行成功時傳回 0, 其他的錯誤代號可用
       cudaGetErrorString() 來解譯.


--------------------------------------------------------
(2)釋放顯示記憶體 cudaFree()        [cuda.h]
--------------------------------------------------------

       cudaError_t cudaFree(void* ptr);

           ptr   指向欲釋放的位址 (device memory)


--------------------------------------------------------
(3)記憶體複制 cudaMemcpy()          [cuda.h]
--------------------------------------------------------

       cudaError_t cudaMemcpy(void* dst, const void* src, size_t count,
                          enum cudaMemcpyKind kind);

           dst   指向目的位址
           src   指向來源位址
           count 拷貝區塊大小 (單位 bytes)
           kind  有四種拷貝流向
                 cudaMemcpyHostToHost       主機 -> 主機
                 cudaMemcpyHostToDevice     主機 -> 裝置
                 cudaMemcpyDeviceToHost     裝置 -> 主機
                 cudaMemcpyDeviceToDevice   裝置 -> 裝置

--------------------------------------------------------
(4)錯誤字串解譯 cudaGetErrorString()   [cuda.h]
--------------------------------------------------------

        const char* cudaGetErrorString(cudaError_t error);

        傳回錯誤代號(error)所代表的字串

--------------------------------------------------------
(5)同步化  cudaThreadSynchronize()  [cuda.h]
--------------------------------------------------------

        cudaError_t cudaThreadSynchronize(void);

        用來進行核心和主機程序的同步     
-------------------------------------------------------------------
3.-----------------------------------------------------------------
CUDA 中，主機函式的寫法與呼叫和傳統 C/C++ 無異，而裝置核心 (kernel) 要使用
延伸語法：

    __global__ void 函式名稱 (函式引數...){
            ...函式內容...
    };

多了 __global__ 這標籤來標明這道函式是核心程式碼，要編譯器特別照顧一下，
註意事項如下：
    (1) 傳回值隻能是 void (要傳東西出來請透過引數)
    (2) 裏面不能呼叫主機函式或 global 函式 (這兩者皆是主機用的)
    (3) 輸入的資料若是位址或參考時，必需指向裝置記憶體。

呼叫 kernel 函式的語法比一般 C 函式多了指定網格和區塊大小的手序：

        函式名稱 <<<網格大小, 區塊大小, (shared memory大小(可忽略))>>> (函式引數...);
-------------------------------------------------------------------------

4.-----------------------------------------------------------------------
網格、區塊、執行緒是 CUDA 中最重要的部份, 必需熟悉

(1) GPU 是具備超多核心，能行大量平行化運算的晶片，執行緒眾多，要分群組管理：
    最基本的執行單位是【執行緒】(thread)，
    數個執行緒組成【區塊】(block)，
    數個區塊組成【網格】(grid)，
    整個網格就是所謂的【核心】(kernel)。

(2)【執行緒】是最基本的執行單位，程式設計師站在執行緒的角度，透過內建變數，
    定出執行緒的位置，對工作進行主動切割。

(3)【區塊】為執行緒的群組，一個區塊可包含 1~512 個執行緒，
    每個執行緒在區塊中擁有唯一的索引編號，記錄於內建變數 threadIdx。
    每個區塊中包含的執行緒數目，記錄於內建變數 blockDim。
    相同區塊內的執行緒可同步化，而且可透過共享記憶體交換資料 (詳見第五、六招)

(4)【網格】為區塊的群組，一個網格可包含 1~65535 個區塊，
    每個區塊在網格中擁有唯一的索引編號，記錄於內建變數 blockIdx。
    每個網格中包含的區塊數目，記錄於內建變數 gridDim。
    網格中的區塊可能會同時或分散在不同時間執行，視硬體情況而定。

(5) 內建唯讀變數 gridDim, blockDim, blockIdx, threadIdx 皆是 3D 正整數的結構體

        uint3 gridDim  ：網格大小   (網格中包含的區塊數目)
        uint3 blockIdx ：區塊索引   (網格中區塊的索引)

        uint3 blockDim ：區塊大小   (區塊中包含的執行緒數目)
        uint3 threadIdx：執行緒索引 (區塊中執行緒的索引)

    其中 uint3 為 3D 的正整數型態，定義如下

         struct uint3{
                 unsigned int x,y,z;
         };

    這些唯讀變數隻能在核心中使用。

    PS:GRID 的大小雖然是 uint3 的結構，但隻能使用 2D 而已(其 z 成員隻能是 1)，BLOCK 才能完整支援 3D 結構。

(6) 核心呼叫時指定的網格和區塊大小對應的就是其中 gridDim 和 blockDim 兩變數

        uint3 gridDim  ：網格大小   (網格中包含的區塊數目)
        uint3 blockDim ：區塊大小   (區塊中包含的執行緒數目)

    可以在呼叫時隻指定一維，此時變數裏面的 y 和 z 成員都等於 1：

        核心名稱<<<int 網格大小, int 區塊大小>>>(引數...);

    也可以指定三維的呼叫:

        核心名稱<<<dim3 網格大小, dim3 區塊大小>>>(引數...);

    或者混合使用:

        核心名稱<<<dim3 網格大小, int 區塊大小>>>(引數...);
        核心名稱<<<int 網格大小, dim3 區塊大小>>>(引數...);

    其中 dim3 等於 uint3，隻是有寫好 constructor 而己。

    dim3 grid( 512 );         // 512 x 1 x 1
    dim3 block( 1024, 1024 ); // 1024 x 1024 x 1
    fooKernel<<< grid, block >>>();
    in c code --> dim3 grid = { 512, 512, 1 };
    in c++ code --> dim3 grid = (512, 512, 1 );	
    dim3 grid(512,512,1) -->最大


(7) 網格和區塊大小在設定時有一定的限制

        網格: max(gridDim)  = 65535
        區塊: max(blockDim) = 512

    實際在用的時候 blockDim 還會有資源上的限制, 主要是暫存器數目,
    所以有時達不到 512 這個數量, 在 3 維的情況還會有其他的限制,
    建議使用 1 維的方式呼叫, 到核心中再去切, 執行緒組態比較簡單,
    而且 bug 和限制也會比較少.
